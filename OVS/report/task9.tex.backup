\section*{Task 9}
Previously, for formal verification we only had experience with (symbolic) model checkers such as NuSMV \url{http://nusmv.fbk.eu/}. This technique was more difficult to use as it had to be done in a separate language. These kind of tools are thus also more suitable as a basis for other verification tools. The advantage of those tools that verification of very complex properties can be done, and counterexamples can be given which provide valuable insight in subtle bugs.

Tools such as \texttt{splint} need annotations to work properly, and therefore require considerable more effort of the programmer.
We also tried to use \texttt{valgrind}, but failed to detect the memory leak as found in Task~4, probably due to faulty usage.

\cite{dkw2008} gives a good overview of the different verification techniques and their advantages and disadvantages. In summary, all these tools make different trade-offs between coverage, soundness, usability, performance, \ldots
\begin{quote}Static analysis techniques based on abstract interpretation
scale well at the cost of limited precision, which manifests
itself in a possibly large number of false warnings. The tool
support is mature. 

Model Checking tools that use abstraction
can check complex safety properties, and are able to generate
counterexamples. Bounded Model Checkers are very strong at
detecting shallow bugs, but are unable to prove even simple
properties if the program contains deep loops. 

The model checking-based tools for software are less robust than static
analysis tools and the market for these tools is in its infancy.\end{quote}

It is important to note that all verification techniques are currently an important topic of research. While some of these tools are very mature and really usable, others are very user-unfriendly and have bad support. Thus these tools should not only be able to detect many kinds of potential vulnerabilities, but they should do it with a low false positive rate, be scalable for real-world applications and give a good explanation or even a solution.

\cite{dkw2008} also points out the shortcomings of most current techniques:
\begin{quote}
The challenges for future research
in software analysis are dynamically allocated data structures,
shared-variable concurrency, and the environment problem.

None of the tools we survey is able to assert even trivial
properties of dynamically allocated data structures, despite the
large body of (theoretical) research in this area. Similarly,
concurrency has been the subject of research for decades,
but the few tools that analyze programs with shared-variable
concurrency still scale poorly. 

Any formal analysis requires a
formal model of the design under test---in case of software,
this model often comprises a non-trivial environment that
the code runs in (libraries, other programs, and hardware
components). As programs often rely on properties of this
environment, substantial manual effort is required to model
these parts of the environment.
\end{quote}

Programmers should not blindly trust these formal verification tools: a successfully verified program can still have severe problems, and sometimes false positives are given. \emph{A fool with a tool is still a fool.} 



